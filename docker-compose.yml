version: '3.8'

networks:
  aegis guard: {}
  atlas:
    internal: true
  sentinel:
    internal: true
  elk: {}

volumes:
  shared_code:
    driver: local
    driver_opts:
      type: none
      device: ./requirements/shared_code
      o: bind
  certifications_data:
    driver: local
  elasticsearch_data:
    driver: local
  logstash_data:
    driver: local
  kibana_data:
    driver: local

services:

  setup:
    image: elasticsearch:8.12.2
    volumes:
      - certifications_data:/usr/share/elasticsearch/config/certificates
    user: "0"
    environment:
      - xpack.security.http.ssl.enabled=true
      - xpack.security.transport.ssl.enabled=true
    command: >
        bash -c '
          if [ x${ELASTIC_PASSWORD} == x ]; then
            echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
            exit 1;
          elif [ x${KIBANA_PASSWORD} == x ]; then
            echo "Set the KIBANA_PASSWORD environment variable in the .env file";
            exit 1;
          fi;
          if [ ! -f config/certificates/ca.zip ]; then
            echo "Creating CA";
            bin/elasticsearch-certutil ca --silent --pem -out config/certificates/ca.zip;
            unzip config/certificates/ca.zip -d config/certificates;
          fi;
          if [ ! -f config/certificates/certificates.zip ]; then
            echo "Creating certificates";
            echo -ne \
            "instances:\n"\
            "  - name: elasticsearch\n"\
            "    dns:\n"\
            "      - elasticsearch\n"\
            "      - localhost\n"\
            "    ip:\n"\
            "      - 127.0.0.1\n"\
            "  - name: kibana\n"\
            "    dns:\n"\
            "      - kibana\n"\
            "      - localhost\n"\
            "    ip:\n"\
            "      - 127.0.0.1\n"\
            > config/certificates/instances.yml;
            bin/elasticsearch-certutil cert --silent --pem -out config/certificates/certificates.zip --in config/certificates/instances.yml --ca-cert config/certificates/ca/ca.crt --ca-key config/certificates/ca/ca.key;
            unzip config/certificates/certificates.zip -d config/certificates;
          fi;
          echo "Setting file permissions"
          chown -R root:root config/certificates;
          find . -type d -exec chmod 750 \{\} \;;
          find . -type f -exec chmod 640 \{\} \;;
          echo "Waiting for Elasticsearch availability";
          until curl -s --cacert config/certificates/ca/ca.crt https://elasticsearch:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
          echo "Setting kibana_system password";
          until curl -s -X POST --cacert config/certificates/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://elasticsearch:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
          echo "All done!";'
    healthcheck:
      test: ["CMD", "curl", "-f", "--cacert", "config/certificates/ca/ca.crt", "https://elasticsearch:9200"]
      interval: 30s
      timeout: 10s
      retries: 5
    ports:
      - "9200:9200"
      - "9300:9300"
    # networks:
    #   - aegis guard

  elasticsearch:
    depends_on:
      setup:
        condition: service_healthy
    container_name: apollo
    image: apollo
    build:
      context: ./requirements/apollo
      dockerfile: Dockerfile
    labels:
      co.elastic.logs/module: elasticsearch
    volumes:
      - certifications_data:/usr/share/elasticsearch/config/certificates
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - cluster.name=${CLUSTER_NAME}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certificates/elasticsearch.key
      - xpack.security.http.ssl.certificate=certificates/elasticsearch.crt
      - xpack.security.http.ssl.certificate_authorities=certificates/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certificates/elasticsearch.key
      - xpack.security.transport.ssl.certificate=certificates/elasticsearch.crt
      - xpack.security.transport.ssl.certificate_authorities=certificates/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certificates/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  kibana:
    depends_on:
      elasticsearch:
        condition: service_healthy
    container_name: iris
    image: iris
    build:
      context: ./requirements/iris
      dockerfile: Dockerfile
    labels:
      co.elastic.logs/module: kibana
    volumes:
      - certifications_data:/usr/share/elasticsearch/config/certificates
      - kibana_data:/usr/share/kibana/data
    ports:
      - "5601:5601"
    environment:
     - SERVERNAME=kibana
     - ELASTICSEARCH_HOSTS=https://elasticsearch:9200
     - ELASTICSEARCH_USERNAME=kibana_system
     - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
     - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
     - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
     - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
     - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
    mem_limit: ${MEM_LIMIT}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  logstash:
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    container_name: thot
    image: thot
    build:
      context: ./requirements/thot
      dockerfile: Dockerfile
    labels:
      co.elastic.logs/module: logstash
    user: root
    volumes:
      - certifications_data:/usr/share/elasticsearch/config/certificates
      - logstash_data:/usr/share/logstash
      - ./requirements/thot/config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
    environment:
      - NODE_NAME="logstash"
      - xpack.monitoring.enabled=false
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://elasticsearch:9200
    command: logstash -f /usr/share/logstash/pipeline/logstash.conf
    ports:
      - "9600:9600"
      - "5044:5044/udp"
    mem_limit: ${MEM_LIMIT}

# Server proxy
  aegis:
    container_name: aegis
    image: aegis
    build:
      context: ./requirements/aegis
      dockerfile: Dockerfile
    ports:
      # - "80:80"
      # - "443:443"
      - "7999:80"
      - "8000:443"
    volumes:
      - ./requirements/aegis:/usr/share/nginx/html
      - /etc/letsencrypt/archive/batch42.me:/etc/letsencrypt/live/batch42.me:r
    networks:
      - aegis guard
      - atlas
      - sentinel
    restart: on-failure

# Profil container
  alfred:
    container_name: alfred
    image: alfred
    build:
      context: ./requirements/alfred
      dockerfile: Dockerfile
    volumes:
      - ./requirements/alfred/alfred_project:/app
      - shared_code:/app/shared
    ports:
      - "8001:8001"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Tournament container
  coubertin:
    container_name: coubertin
    image: coubertin
    build:
      context: ./requirements/coubertin
      dockerfile: Dockerfile
    volumes:
      - ./requirements/coubertin/coubertin_project:/app
      - shared_code:/app/shared
    ports:
      - "8002:8002"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Matchmaking container
  cupidon:
    container_name: cupidon
    image: cupidon
    build:
      context: ./requirements/cupidon
      dockerfile: Dockerfile
    volumes:
      - ./requirements/cupidon/cupidon_project:/app
      - shared_code:/app/shared
    ports:
      - "8003:8003"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Matchmaking container
  davinci:
    container_name: davinci
    image: davinci
    build:
      context: ./requirements/davinci
      dockerfile: Dockerfile
    volumes:
      - shared_code:/app/shared
    ports:
      - "8010:8010"
    depends_on:
      - aegis
    networks:
      - sentinel
    restart: on-failure

# Notification container
  hermes:
    container_name: hermes
    image: hermes
    build:
      context: ./requirements/hermes
      dockerfile: Dockerfile
    volumes:
      - ./requirements/hermes/hermes_project:/app
      - shared_code:/app/shared
    ports:
      - "8004:8004"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Auth inter-container container
  lovelace:
    container_name: lovelace
    image: lovelace
    build:
      context: ./requirements/lovelace
      dockerfile: Dockerfile
    volumes:
      - ./requirements/lovelace/lovelace_project:/app
      - shared_code:/app/shared
    ports:
      - "8005:8005"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Game container
  ludo:
    container_name: ludo
    image: ludo
    build:
      context: ./requirements/ludo
      dockerfile: Dockerfile
    volumes:
      - ./requirements/ludo/ludo_project:/app
      - shared_code:/app/shared
    ports:
      - "8006:8006"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Front container
  malevitch:
    container_name: malevitch
    image: malevitch
    build:
      context: ./requirements/malevitch
      dockerfile: Dockerfile
    volumes:
      - ./requirements/malevitch:/usr/share/nginx/html
    ports:
      - "8007:80"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Prometheus container
  mensura:
    container_name: mensura
    image: mensura
    build:
      context: ./requirements/mensura
      dockerfile: Dockerfile
    volumes:
      - shared_code:/app/shared
    ports:
      - "8011:8011"
    depends_on:
      - aegis
    networks:
      - atlas
      - sentinel
    restart: on-failure

# Stats container
  mnemosine:
    container_name: mnemosine
    image: mnemosine
    build:
      context: ./requirements/mnemosine
      dockerfile: Dockerfile
    volumes:
      - ./requirements/mnemosine/mnemosine_project:/app
      - shared_code:/app/shared
    ports:
      - "8008:8008"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Authentification container
  petrus:
    container_name: petrus
    image: petrus
    build:
      context: ./requirements/petrus
      dockerfile: Dockerfile
    volumes:
      - ./requirements/petrus/petrus_project:/app
      - shared_code:/app/shared
    ports:
      - "8009:8009"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Data exporter container
  mensura_exporter:
    container_name: mensura_exporter
    image: quay.io/prometheus/node-exporter
    volumes:
      - "/:/host:ro,rslave"
    ports:
      - "9100:9100"
    command: ["--path.rootfs=/host"]
    networks:
      - atlas
    pid: "host"
    depends_on:
      - aegis
    restart: on-failure

# Data exporter container
  aegis_exporter:
    container_name: aegis_exporter
    image: aegis_exporter
    build:
      context: ./requirements/mensura/aegis_exporter
      dockerfile: Dockerfile
    volumes:
      - shared_code:/app/shared
    ports:
      - "9913:9913"
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure

# Websocket container
  redis:
    container_name: redis
    image: 'bitnami/redis:latest'
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    depends_on:
      - aegis
    networks:
      - atlas
    restart: on-failure
